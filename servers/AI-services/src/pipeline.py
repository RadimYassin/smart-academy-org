"""
EduPath-MS: Pipeline Data Science pour Learning Analytics
Auteur: Pipeline automatis√© pour analyse √©ducative
Date: 2025-12-21

Ce script impl√©mente 4 composants principaux:
1. PrepaData: Nettoyage et Feature Engineering
2. StudentProfiler: Clustering K-Means (Non supervis√©)
3. PathPredictor: Pr√©diction XGBoost (Supervis√©)
4. RecoBuilder: Recommandations P√©dagogiques (Optionnel - n√©cessite OpenAI API)

Infrastructure:
- PostgreSQL: Stockage des donn√©es (mode hybride CSV/PostgreSQL)
- MLflow: Tracking des exp√©riences ML
- Airflow: Orchestration du pipeline (voir airflow/dags/)
"""

# ============================================================================
# IMPORTS
# ============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, silhouette_score
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Configuration centralis√©e
try:
    from config import *
    from database import save_data, load_data, init_db
    from mlflow_config import init_mlflow, MLflowRun, log_params, log_metrics, log_model
except ImportError:
    from src.config import *
    from src.database import save_data, load_data, init_db
    from src.mlflow_config import init_mlflow, MLflowRun, log_params, log_metrics, log_model

# Configuration du style des graphiques
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

# ============================================================================
# COMPOSANT 1: PrepaData - Nettoyage et Feature Engineering
# ============================================================================

class PrepaData:
    """
    Classe pour le nettoyage et la pr√©paration des donn√©es √©tudiantes.
    
    Fonctionnalit√©s:
    - Recalcul de la colonne Total
    - Encodage des mati√®res (Subject) en arabe
    - Cr√©ation de la variable cible is_fail
    """
    
    def __init__(self, df):
        """
        Initialise avec un DataFrame
        
        Args:
            df: DataFrame pandas avec les colonnes requises
        """
        self.df = df.copy()
        self.label_encoder = LabelEncoder()
        
    def recalculate_total(self):
        """
        Recalcule la colonne Total = Practical + Theoretical.
        Si les deux sont 0 et Status = "Absent", Total reste 0.
        """
        print("üìä Recalcul de la colonne 'Total'...")
        
        # Cr√©er une nouvelle colonne Total calcul√©e
        self.df['Total_Calculated'] = self.df['Practical'] + self.df['Theoretical']
        
        # Remplacer NaN dans Total par les valeurs calcul√©es
        self.df['Total'] = self.df['Total'].fillna(self.df['Total_Calculated'])
        
        # Supprimer la colonne temporaire
        self.df.drop('Total_Calculated', axis=1, inplace=True)
        
        print(f"‚úì Total recalcul√©. Valeurs NaN restantes: {self.df['Total'].isna().sum()}")
        
        return self
    
    def encode_subject(self):
        """
        Encode la colonne Subject (contenant du texte arabe) en valeurs num√©riques.
        Utilise LabelEncoder pour transformer chaque mati√®re unique en un entier.
        """
        print("üî§ Encodage de la colonne 'Subject' (texte arabe)...")
        
        # G√©rer les valeurs manquantes
        self.df['Subject'] = self.df['Subject'].fillna('Unknown')
        
        # Encoder les mati√®res
        self.df['Subject_Encoded'] = self.label_encoder.fit_transform(self.df['Subject'])
        
        # Afficher quelques exemples de mapping
        unique_subjects = self.df['Subject'].unique()[:5]
        print(f"‚úì Encodage termin√©. {len(self.df['Subject'].unique())} mati√®res uniques.")
        print("Exemples de mapping:")
        for subject in unique_subjects:
            encoded_val = self.df[self.df['Subject'] == subject]['Subject_Encoded'].iloc[0]
            print(f"  - {subject} ‚Üí {encoded_val}")
        
        return self
    
    def create_target_variable(self, threshold=10):
        """
        Cr√©e la variable cible binaire 'is_fail'.
        
        is_fail = 1 si:
        - Status est "Withdrawal", "Debarred" ou "Absent"
        - OU Total < threshold (d√©faut: 10 pour validation minimum)
        
        Args:
            threshold: Seuil de note minimum pour la r√©ussite (d√©faut: 10)
        """
        print(f"üéØ Cr√©ation de la variable cible 'is_fail' (seuil: {threshold})...")
        
        # D√©finir les statuts d'√©chec
        failure_statuses = ['Withdrawal', 'Debarred', 'Absent']
        
        # Cr√©er la colonne is_fail
        self.df['is_fail'] = 0
        
        # Marquer comme √©chec si Status dans la liste d'√©chec
        self.df.loc[self.df['Status'].isin(failure_statuses), 'is_fail'] = 1
        
        # Marquer comme √©chec si Total < threshold
        self.df.loc[self.df['Total'] < threshold, 'is_fail'] = 1
        
        # Statistiques
        fail_count = self.df['is_fail'].sum()
        total_count = len(self.df)
        fail_rate = (fail_count / total_count) * 100
        
        print(f"‚úì Variable cible cr√©√©e:")
        print(f"  - √âchecs (is_fail=1): {fail_count} ({fail_rate:.2f}%)")
        print(f"  - R√©ussites (is_fail=0): {total_count - fail_count} ({100-fail_rate:.2f}%)")
        
        return self
    
    def get_clean_data(self):
        """
        Retourne le DataFrame nettoy√© et pr√©par√©.
        """
        return self.df
    
    def run_all(self, threshold=10):
        """
        Ex√©cute toutes les √©tapes de pr√©paration.
        
        Args:
            threshold: Seuil de note pour is_fail
        
        Returns:
            DataFrame nettoy√©
        """
        print("\n" + "="*70)
        print("üîß COMPOSANT 1: PrepaData - Nettoyage et Feature Engineering")
        print("="*70 + "\n")
        
        self.recalculate_total()
        self.encode_subject()
        self.create_target_variable(threshold)
        
        print("\n‚úÖ Pr√©paration termin√©e!")
        return self.df


# ============================================================================
# COMPOSANT 2: StudentProfiler - Clustering K-Means
# ============================================================================

class StudentProfiler:
    """
    Classe pour cr√©er des profils d'√©tudiants via clustering K-Means.
    
    Fonctionnalit√©s:
    - Agr√©gation des donn√©es par √©tudiant (ID)
    - Normalisation avec StandardScaler
    - M√©thode du coude pour trouver K optimal
    - Clustering K-Means
    """
    
    def __init__(self, df):
        """
        Initialise avec un DataFrame pr√©par√©
        
        Args:
            df: DataFrame apr√®s PrepaData
        """
        self.df = df.copy()
        self.scaler = StandardScaler()
        self.student_features = None
        self.scaled_features = None
        self.kmeans = None
        
    def aggregate_by_student(self):
        """
        Agr√®ge les donn√©es par ID √©tudiant pour cr√©er des statistiques globales:
        - Moyenne g√©n√©rale
        - Nombre d'absences
        - Taux d'√©chec par semestre
        """
        print("üìà Agr√©gation des donn√©es par √©tudiant...")
        
        # Convertir ID en num√©rique et supprimer les valeurs invalides
        self.df['ID'] = pd.to_numeric(self.df['ID'], errors='coerce')
        self.df = self.df.dropna(subset=['ID'])
        self.df['ID'] = self.df['ID'].astype(int)
        
        # Grouper par ID
        student_agg = self.df.groupby('ID').agg({
            'Total': 'mean',                    # Moyenne g√©n√©rale
            'is_fail': 'sum',                   # Nombre total d'√©checs
            'Semester': 'count',                # Nombre de cours suivis
            'Practical': 'mean',                # Moyenne pratique
            'Theoretical': 'mean'               # Moyenne th√©orique
        }).reset_index()
        
        # Renommer les colonnes
        student_agg.columns = ['ID', 'Average_Grade', 'Total_Failures', 
                                'Total_Courses', 'Avg_Practical', 'Avg_Theoretical']
        
        # Calculer le taux d'√©chec
        student_agg['Failure_Rate'] = (student_agg['Total_Failures'] / 
                                        student_agg['Total_Courses']) * 100
        
        # Compter les absences (statut="Absent")
        absence_count = self.df[self.df['Status'] == 'Absent'].groupby('ID').size()
        student_agg['Absence_Count'] = student_agg['ID'].map(absence_count).fillna(0)
        
        self.student_features = student_agg
        
        print(f"‚úì Agr√©gation termin√©e. {len(student_agg)} √©tudiants uniques.")
        print(f"\nStatistiques par √©tudiant:")
        print(student_agg.describe())
        
        return self
    
    def normalize_features(self):
        """
        Normalise les features num√©riques avec StandardScaler.
        """
        print("\nüîÑ Normalisation des features...")
        
        # S√©lectionner les features num√©riques (exclure ID)
        feature_cols = ['Average_Grade', 'Total_Failures', 'Total_Courses', 
                        'Avg_Practical', 'Avg_Theoretical', 'Failure_Rate', 'Absence_Count']
        
        X = self.student_features[feature_cols]
        
        # Normaliser
        self.scaled_features = self.scaler.fit_transform(X)
        
        print(f"‚úì Normalisation termin√©e. Shape: {self.scaled_features.shape}")
        
        return self
    
    def find_optimal_k(self, k_range=range(2, 8)):
        """
        Utilise la m√©thode du coude (Elbow Method) pour trouver le K optimal.
        
        Args:
            k_range: Range de valeurs K √† tester (d√©faut: 2 √† 7)
        """
        print(f"\nüìä Recherche du K optimal (m√©thode du coude)...")
        
        inertias = []
        silhouette_scores = []
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(self.scaled_features)
            inertias.append(kmeans.inertia_)
            
            # Calculer le silhouette score
            score = silhouette_score(self.scaled_features, kmeans.labels_)
            silhouette_scores.append(score)
        
        # Visualisation
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
        
        # Elbow curve
        ax1.plot(k_range, inertias, marker='o', linewidth=2, markersize=8)
        ax1.set_xlabel('Nombre de clusters (K)', fontsize=12)
        ax1.set_ylabel('Inertie (Within-cluster sum of squares)', fontsize=12)
        ax1.set_title('M√©thode du Coude pour K optimal', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # Silhouette scores
        ax2.plot(k_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='orange')
        ax2.set_xlabel('Nombre de clusters (K)', fontsize=12)
        ax2.set_ylabel('Silhouette Score', fontsize=12)
        ax2.set_title('Silhouette Score par K', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(ELBOW_PLOT, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úì Graphique sauvegard√©: elbow_method.png")
        print(f"\nSilhouette Scores:")
        for k, score in zip(k_range, silhouette_scores):
            print(f"  K={k}: {score:.4f}")
        
        return self
    
    def cluster_students(self, n_clusters=4):
        """
        Applique K-Means clustering avec K clusters.
        
        Args:
            n_clusters: Nombre de clusters (d√©faut: 4)
        """
        print(f"\nüéØ Application de K-Means avec K={n_clusters}...")
        
        # Appliquer K-Means
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = self.kmeans.fit_predict(self.scaled_features)
        
        # Ajouter les clusters au DataFrame
        self.student_features['Cluster'] = clusters
        
        # Analyser les clusters
        print(f"\n‚úì Clustering termin√©. Distribution des clusters:")
        print(self.student_features['Cluster'].value_counts().sort_index())
        
        # Profil de chaque cluster
        print(f"\nüìã Profil moyen par cluster:")
        cluster_profiles = self.student_features.groupby('Cluster').mean()
        print(cluster_profiles)
        
        # Interpr√©ter les clusters
        self._interpret_clusters()
        
        return self
    
    def _interpret_clusters(self):
        """
        Interpr√®te les clusters en leur donnant des labels significatifs.
        """
        print(f"\nüè∑Ô∏è Interpr√©tation des clusters:")
        
        for cluster_id in sorted(self.student_features['Cluster'].unique()):
            cluster_data = self.student_features[self.student_features['Cluster'] == cluster_id]
            
            avg_grade = cluster_data['Average_Grade'].mean()
            failure_rate = cluster_data['Failure_Rate'].mean()
            absence_count = cluster_data['Absence_Count'].mean()
            
            # D√©terminer le profil
            if failure_rate > 60 or absence_count > 5:
                profile = "üî¥ En grande difficult√© / D√©crocheurs"
            elif failure_rate > 30:
                profile = "üü† En difficult√©"
            elif avg_grade > 14:
                profile = "üü¢ Excellents"
            else:
                profile = "üü° Moyens / Stables"
            
            print(f"\n  Cluster {cluster_id} - {profile}")
            print(f"    - Moyenne g√©n√©rale: {avg_grade:.2f}")
            print(f"    - Taux d'√©chec: {failure_rate:.2f}%")
            print(f"    - Absences moyennes: {absence_count:.2f}")
            print(f"    - Nombre d'√©tudiants: {len(cluster_data)}")
    
    def visualize_clusters(self):
        """
        Visualise les clusters en 2D (PCA ou features principales).
        """
        from sklearn.decomposition import PCA
        
        print(f"\nüìä Visualisation des clusters...")
        
        # R√©duction √† 2D avec PCA
        pca = PCA(n_components=2)
        features_2d = pca.fit_transform(self.scaled_features)
        
        plt.figure(figsize=(10, 7))
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], 
                            c=self.student_features['Cluster'], 
                            cmap='viridis', s=50, alpha=0.6, edgecolors='black')
        plt.colorbar(scatter, label='Cluster')
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)
        plt.title('Profils d\'√©tudiants - Clustering K-Means (PCA)', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(CLUSTERS_PLOT, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úì Graphique sauvegard√©: student_clusters.png")
        
        return self
    
    def get_student_profiles(self):
        """
        Retourne le DataFrame avec les profils √©tudiants et clusters.
        """
        return self.student_features
    
    def run_all(self, n_clusters=4):
        """
        Ex√©cute toutes les √©tapes de profiling.
        
        Args:
            n_clusters: Nombre de clusters √† cr√©er
        
        Returns:
            DataFrame avec profils √©tudiants
        """
        print("\n" + "="*70)
        print("üë• COMPOSANT 2: StudentProfiler - Clustering K-Means")
        print("="*70 + "\n")
        
        self.aggregate_by_student()
        self.normalize_features()
        self.find_optimal_k()
        self.cluster_students(n_clusters)
        self.visualize_clusters()
        
        print("\n‚úÖ Profiling termin√©!")
        return self.student_features


# ============================================================================
# COMPOSANT 3: PathPredictor - Pr√©diction XGBoost
# ============================================================================

class PathPredictor:
    """
    Classe pour pr√©dire la r√©ussite/√©chec avec XGBoost.
    
    Fonctionnalit√©s:
    - Pr√©paration des features (X) et target (y)
    - Entra√Ænement XGBoost avec gestion du d√©s√©quilibre
    - √âvaluation (confusion matrix, feature importance)
    """
    
    def __init__(self, df):
        """
        Initialise avec un DataFrame pr√©par√©
        
        Args:
            df: DataFrame apr√®s PrepaData
        """
        self.df = df.copy()
        self.model = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.feature_names = None
        
    def prepare_features(self):
        """
        Pr√©pare les features (X) et la target (y = is_fail) avec feature engineering avanc√©.
        """
        print("üîß Pr√©paration des features avec feature engineering avanc√©...")
        
        # Features de base
        feature_cols = ['Subject_Encoded', 'Semester', 'Practical', 
                        'Theoretical', 'Total', 'MajorYear']
        
        # Encoder Major si n√©cessaire
        if self.df['Major'].dtype == 'object':
            le_major = LabelEncoder()
            self.df['Major_Encoded'] = le_major.fit_transform(self.df['Major'].fillna('Unknown'))
            feature_cols.append('Major_Encoded')
        
        # === FEATURE ENGINEERING AVANC√â ===
        
        # 1. Ratio Pratique/Th√©orique
        self.df['Practical_Theoretical_Ratio'] = self.df['Practical'] / (self.df['Theoretical'] + 1e-5)
        feature_cols.append('Practical_Theoretical_Ratio')
        
        # 2. √âcart √† la moyenne
        self.df['Total_Deviation'] = self.df['Total'] - self.df['Total'].mean()
        feature_cols.append('Total_Deviation')
        
        # 3. Performance relative par mati√®re
        subject_means = self.df.groupby('Subject_Encoded')['Total'].transform('mean')
        self.df['Subject_Relative_Performance'] = self.df['Total'] - subject_means
        feature_cols.append('Subject_Relative_Performance')
        
        # 4. Interaction Semester x Subject (difficult√© croissante)
        self.df['Semester_Subject_Interaction'] = self.df['Semester'] * self.df['Subject_Encoded']
        feature_cols.append('Semester_Subject_Interaction')
        
        # 5. Indicateur de tr√®s faible note
        self.df['Very_Low_Score'] = (self.df['Total'] < 5).astype(int)
        feature_cols.append('Very_Low_Score')
        
        # 6. Progression (diff√©rence avec semestre pr√©c√©dent par √©tudiant)
        if 'ID' in self.df.columns:
            self.df = self.df.sort_values(['ID', 'Semester'])
            self.df['Previous_Total'] = self.df.groupby('ID')['Total'].shift(1)
            self.df['Score_Progression'] = self.df['Total'] - self.df['Previous_Total']
            self.df['Score_Progression'] = self.df['Score_Progression'].fillna(0)
            feature_cols.append('Score_Progression')
        
        # G√©rer les valeurs manquantes et infinies
        self.df[feature_cols] = self.df[feature_cols].fillna(0)
        self.df[feature_cols] = self.df[feature_cols].replace([np.inf, -np.inf], 0)
        
        # X et y
        X = self.df[feature_cols]
        y = self.df['is_fail']
        
        # Split train/test (80% train, 20% test)
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        self.feature_names = feature_cols
        
        print(f"‚úì Features pr√©par√©es (avec engineering):")
        print(f"  - Train: {self.X_train.shape}")
        print(f"  - Test: {self.X_test.shape}")
        print(f"  - Features totales: {len(feature_cols)} ({len(feature_cols) - 6} nouvelles)")
        print(f"  - Nouvelles features: Ratios, D√©viations, Interactions, Progression")
        
        # Distribution de la target
        print(f"\n  Distribution de la target:")
        print(f"    - Train: √âchecs={self.y_train.sum()}, R√©ussites={len(self.y_train)-self.y_train.sum()}")
        print(f"    - Test: √âchecs={self.y_test.sum()}, R√©ussites={len(self.y_test)-self.y_test.sum()}")
        
        return self
    
    def train_model(self, use_grid_search=True):
        """
        Entra√Æne le mod√®le XGBoost avec hyperparameter tuning pour atteindre 90%+ accuracy.
        
        Args:
            use_grid_search: Si True, utilise GridSearchCV pour optimiser (plus lent mais meilleur)
        """
        print(f"\nüöÄ Entra√Ænement du mod√®le XGBoost avec optimisation...")
        
        # Calculer scale_pos_weight pour g√©rer le d√©s√©quilibre
        negative_count = (self.y_train == 0).sum()
        positive_count = (self.y_train == 1).sum()
        scale_pos_weight = negative_count / positive_count
        
        print(f"  - D√©s√©quilibre d√©tect√©: Ratio={negative_count}/{positive_count}")
        print(f"  - scale_pos_weight={scale_pos_weight:.2f}")
        
        if use_grid_search:
            print(f"  - Mode: Hyperparameter Tuning (GridSearchCV)")
            print(f"  - Cela peut prendre 2-3 minutes...")
            
            from sklearn.model_selection import GridSearchCV
            
            # Mod√®le de base
            base_model = xgb.XGBClassifier(
                scale_pos_weight=scale_pos_weight,
                random_state=42,
                eval_metric='logloss'
            )
            
            # Grille d'hyperparam√®tres optimis√©e pour 90%+
            param_grid = {
                'max_depth': [5, 6, 7, 8],
                'learning_rate': [0.05, 0.1, 0.15],
                'n_estimators': [100, 150, 200],
                'min_child_weight': [1, 3, 5],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0]
            }
            
            # GridSearchCV avec cross-validation
            grid_search = GridSearchCV(
                base_model,
                param_grid,
                cv=5,
                scoring='accuracy',
                n_jobs=-1,
                verbose=1
            )
            
            grid_search.fit(self.X_train, self.y_train)
            
            self.model = grid_search.best_estimator_
            
            print(f"\n  ‚úì Meilleurs hyperparam√®tres trouv√©s:")
            for param, value in grid_search.best_params_.items():
                print(f"    - {param}: {value}")
            print(f"  ‚úì Meilleur score CV: {grid_search.best_score_*100:.2f}%")
            
        else:
            # Configuration optimis√©e manuellement
            print(f"  - Mode: Configuration optimis√©e")
            
            self.model = xgb.XGBClassifier(
                max_depth=7,
                learning_rate=0.1,
                n_estimators=150,
                min_child_weight=3,
                subsample=0.9,
                colsample_bytree=0.9,
                scale_pos_weight=scale_pos_weight,
                random_state=42,
                eval_metric='logloss'
            )
            
            self.model.fit(self.X_train, self.y_train)
        
        print(f"\n‚úì Mod√®le XGBoost optimis√© entra√Æn√©!")
        
        return self
    
    def evaluate_model(self):
        """
        √âvalue le mod√®le sur le set de test.
        Affiche la confusion matrix et le classification report.
        """
        print(f"\nüìä √âvaluation du mod√®le...")
        
        # Pr√©dictions
        y_pred_train = self.model.predict(self.X_train)
        y_pred_test = self.model.predict(self.X_test)
        
        # Accuracies
        train_acc = (y_pred_train == self.y_train).mean()
        test_acc = (y_pred_test == self.y_test).mean()
        
        print(f"\n  Accuracy:")
        print(f"    - Train: {train_acc*100:.2f}%")
        print(f"    - Test: {test_acc*100:.2f}%")
        
        # Confusion Matrix
        cm = confusion_matrix(self.y_test, y_pred_test)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['R√©ussite', '√âchec'],
                    yticklabels=['R√©ussite', '√âchec'], cbar=True, annot_kws={"size": 14})
        plt.xlabel('Pr√©diction', fontsize=12)
        plt.ylabel('R√©alit√©', fontsize=12)
        plt.title('Matrice de Confusion - Pr√©diction R√©ussite/√âchec', 
                  fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(CONFUSION_MATRIX_PLOT, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"\n‚úì Matrice de confusion sauvegard√©e: confusion_matrix.png")
        
        # Classification Report
        print(f"\n  Classification Report:")
        print(classification_report(self.y_test, y_pred_test, 
                                    target_names=['R√©ussite', '√âchec']))
        
        return self
    
    def plot_feature_importance(self):
        """
        Affiche l'importance des features pour comprendre les facteurs d'√©chec.
        """
        print(f"\nüìà Importance des features...")
        
        # R√©cup√©rer les importances
        importances = self.model.feature_importances_
        
        # Cr√©er un DataFrame pour faciliter la visualisation
        feature_importance_df = pd.DataFrame({
            'Feature': self.feature_names,
            'Importance': importances
        }).sort_values('Importance', ascending=False)
        
        print(feature_importance_df)
        
        # Visualisation
        plt.figure(figsize=(10, 6))
        plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], 
                 color='steelblue', edgecolor='black')
        plt.xlabel('Importance', fontsize=12)
        plt.ylabel('Features', fontsize=12)
        plt.title('Importance des Features - Pr√©diction d\'√âchec', 
                  fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        plt.savefig(FEATURE_IMPORTANCE_PLOT, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"‚úì Graphique sauvegard√©: feature_importance.png")
        
        return self
    
    def run_all(self, use_grid_search=True):
        """
        Ex√©cute toutes les √©tapes de pr√©diction.
        
        Args:
            use_grid_search: Si True, utilise GridSearchCV (d√©faut: True)
        
        Returns:
            Mod√®le entra√Æn√©
        """
        print("\n" + "="*70)
        print("üéØ COMPOSANT 3: PathPredictor - Pr√©diction XGBoost")
        print("="*70 + "\n")
        
        self.prepare_features()
        self.train_model(use_grid_search=use_grid_search)  # GridSearch par d√©faut
        self.evaluate_model()
        self.plot_feature_importance()
        
        print("\n‚úÖ Pr√©diction termin√©e!")
        return self.model


# ============================================================================
# PIPELINE PRINCIPAL
# ============================================================================

def main():
    """
    Fonction principale qui ex√©cute le pipeline complet.
    """
    print("\n" + "="*70)
    print("üéì EDUPATH-MS: Pipeline Data Science - Learning Analytics")
    print("="*70 + "\n")
    
    # Charger les donn√©es
    print("üìÇ Chargement des donn√©es...")
    df1 = pd.read_csv(DATASET_1)
    df2 = pd.read_csv(DATASET_2)
    
    print(f"  - Dataset 1: {df1.shape}")
    print(f"  - Dataset 2: {df2.shape}")
    
    # Combiner les datasets (ou travailler s√©par√©ment)
    df_combined = pd.concat([df1, df2], ignore_index=True)
    print(f"  - Dataset combin√©: {df_combined.shape}")
    
    # ========================================================================
    # √âTAPE 1: PrepaData
    # ========================================================================
    preparer = PrepaData(df_combined)
    df_clean = preparer.run_all(threshold=DEFAULT_FAIL_THRESHOLD)
    
    # Sauvegarder les donn√©es nettoy√©es (PostgreSQL ou CSV selon config)
    save_data(df_clean, 'cleaned_data', CLEANED_DATA)
    print(f"\nüíæ Donn√©es nettoy√©es sauvegard√©es")
    
    # ========================================================================
    # √âTAPE 2: StudentProfiler
    # ========================================================================
    profiler = StudentProfiler(df_clean)
    student_profiles = profiler.run_all(n_clusters=DEFAULT_N_CLUSTERS)
    
    # Sauvegarder les profils (PostgreSQL ou CSV selon config)
    save_data(student_profiles, 'student_profiles', STUDENT_PROFILES)
    print(f"\nüíæ Profils √©tudiants sauvegard√©s")
    
    # ========================================================================
    # √âTAPE 3: PathPredictor (avec MLflow tracking)
    # ========================================================================
    
    # Initialiser MLflow si disponible
    mlflow_available = init_mlflow()
    
    if mlflow_available:
        # Avec MLflow tracking
        with MLflowRun("path_predictor_run"):
            predictor = PathPredictor(df_clean)
            model = predictor.run_all()
            
            # Logger le mod√®le dans MLflow
            log_model(model, "xgboost_model")
    else:
        # Sans MLflow
        predictor = PathPredictor(df_clean)
        model = predictor.run_all()
    
    # Sauvegarder le mod√®le (fallback pickle)
    import pickle
    with open(XGBOOST_MODEL, 'wb') as f:
        pickle.dump(model, f)
    print(f"\nüíæ Mod√®le XGBoost sauvegard√©: {XGBOOST_MODEL}")
    
    # ========================================================================
    # √âTAPE 4 (OPTIONNELLE): RecoBuilder
    # ========================================================================
    # Pour activer le composant RecoBuilder:
    # 1. Cr√©ez un fichier .env avec votre cl√© OpenAI: OPENAI_API_KEY=sk-...
    # 2. D√©commentez le code ci-dessous
    
    # try:
    #     from recobuilder import RecoBuilder
    #     recommender = RecoBuilder()
    #     recommendations = recommender.run_all(
    #         resources_path=EDUCATIONAL_RESOURCES,
    #         df_clean=df_clean,
    #         df_profiles=student_profiles
    #     )
    #     recommender.save_recommendations(recommendations, RECOMMENDATIONS_OUTPUT)
    #     print(f"\nüíæ Recommandations sauvegard√©es: {RECOMMENDATIONS_OUTPUT}")
    # except Exception as e:
    #     print(f"\n‚ö†Ô∏è RecoBuilder d√©sactiv√©: {e}")
    #     print("   Pour activer, ajoutez votre cl√© OpenAI dans .env")
    
    # ========================================================================
    # R√âSUM√â FINAL
    # ========================================================================
    print("\n" + "="*70)
    print("‚úÖ PIPELINE COMPLET TERMIN√â!")
    print("="*70)
    print("\nüìÅ Fichiers g√©n√©r√©s:")
    print(f"  1. {CLEANED_DATA}")
    print(f"  2. {STUDENT_PROFILES}")
    print(f"  3. {ELBOW_PLOT}")
    print(f"  4. {CLUSTERS_PLOT}")
    print(f"  5. {CONFUSION_MATRIX_PLOT}")
    print(f"  6. {FEATURE_IMPORTANCE_PLOT}")
    print(f"  7. {XGBOOST_MODEL}")
    print("\nüí° Pour g√©n√©rer des recommandations personnalis√©es:")
    print("   python demo_recobuilder.py")
    print("\nüéâ Tous les composants ont √©t√© ex√©cut√©s avec succ√®s!")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
