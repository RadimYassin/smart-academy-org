"""
Wrapper √©l√©gant pour Ollama
Interface propre avec l'API Ollama sans d√©pendances conflictuelles
"""

import requests
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class OllamaChat:
    """
    Wrapper √©l√©gant pour Ollama compatible avec l'interface LangChain
    """
    
    def __init__(self, model: str, base_url: str = "http://localhost:11434", temperature: float = 0.7):
        """
        Initialise le client Ollama
        
        Args:
            model: Nom du mod√®le (ex: llama3)
            base_url: URL de base d'Ollama
            temperature: Temp√©rature de g√©n√©ration (0.0 √† 1.0)
        """
        self.model = model
        self.base_url = base_url.rstrip('/')
        self.temperature = temperature
        logger.info(f"ü§ñ Client Ollama initialis√©: {model} @ {base_url}")
    
    def invoke(self, input_data: Any, config: Optional[Dict] = None, **kwargs) -> Any:
        """
        Invoque le mod√®le Ollama avec un prompt
        Compatible avec l'interface LangChain
        
        Args:
            input_data: Peut √™tre une string, un dict, ou un objet avec messages
            config: Configuration optionnelle (ignor√©e pour compatibilit√©)
            **kwargs: Arguments suppl√©mentaires (ignor√©s pour compatibilit√©)
            
        Returns:
            Objet r√©ponse avec attribut 'content'
        """
        # Extraire le prompt depuis l'input
        if isinstance(input_data, str):
            prompt = input_data
        elif isinstance(input_data, dict):
            # Format dict avec input ou messages
            if 'input' in input_data:
                prompt = input_data['input']
            elif 'messages' in input_data:
                prompt = self._format_messages(input_data['messages'])
            else:
                prompt = str(input_data)
        elif hasattr(input_data, 'messages'):
            # Format LangChain messages
            prompt = self._format_messages(input_data.messages)
        elif hasattr(input_data, 'to_messages'):
            # Format LangChain ChatPromptValue
            messages = input_data.to_messages()
            prompt = self._format_langchain_messages(messages)
        else:
            prompt = str(input_data)
        
        logger.debug(f"Envoi prompt √† Ollama ({len(prompt)} chars)")
        
        # Appeler l'API Ollama
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": self.temperature,
                        "num_predict": 2000  # Limite de tokens pour la r√©ponse
                    }
                },
                timeout=300
            )
            response.raise_for_status()
            result = response.json()
            
            # Cr√©er un objet r√©ponse compatible LangChain
            class ChatResponse:
                def __init__(self, content: str):
                    self.content = content
                    # Attributs additionnels pour compatibilit√© LangChain
                    self.response_metadata = {}
                    self.type = "ai"
            
            return ChatResponse(result["response"])
            
        except requests.exceptions.ConnectionError:
            logger.error("‚ùå Impossible de se connecter √† Ollama. Assurez-vous qu'Ollama est d√©marr√©.")
            raise ConnectionError(
                "Impossible de se connecter √† Ollama. "
                "Assurez-vous qu'Ollama est d√©marr√© (il devrait tourner en arri√®re-plan apr√®s l'installation)."
            )
        except requests.exceptions.Timeout:
            logger.error("‚ùå Timeout lors de l'appel √† Ollama")
            raise TimeoutError("Le mod√®le Ollama met trop de temps √† r√©pondre")
        except Exception as e:
            logger.error(f"‚ùå Erreur Ollama: {str(e)}")
            raise
    
    def _format_messages(self, messages: List[Dict]) -> str:
        """
        Formate une liste de messages dict en prompt texte
        
        Args:
            messages: Liste de messages avec 'role' et 'content'
            
        Returns:
            Prompt format√©
        """
        formatted = []
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'system':
                formatted.append(f"System: {content}")
            elif role == 'assistant':
                formatted.append(f"Assistant: {content}")
            else:
                formatted.append(f"User: {content}")
        
        return "\n\n".join(formatted)
    
    def _format_langchain_messages(self, messages: List) -> str:
        """
        Formate des messages LangChain (objets BaseMessage) en prompt texte
        
        Args:
            messages: Liste d'objets BaseMessage de LangChain
            
        Returns:
            Prompt format√©
        """
        formatted = []
        for msg in messages:
            # Les messages LangChain ont un attribut 'content' et 'type'
            content = getattr(msg, 'content', str(msg))
            msg_type = getattr(msg, 'type', 'human')
            
            if msg_type == 'system':
                formatted.append(f"System: {content}")
            elif msg_type in ['ai', 'assistant']:
                formatted.append(f"Assistant: {content}")
            else:
                formatted.append(f"User: {content}")
        
        return "\n\n".join(formatted)
    
    def bind(self, **kwargs):
        """
        M√©thode bind pour compatibilit√© LangChain
        Retourne self car notre wrapper ne supporte pas le binding avanc√©
        """
        return self
    
    def with_config(self, config: Dict) -> "OllamaChat":
        """
        M√©thode with_config pour compatibilit√© LangChain
        Retourne self car la config est g√©r√©e diff√©remment
        """
        return self

