"""
Moteur RAG (Retrieval Augmented Generation)
Charge l'index FAISS et configure la cha√Æne LangChain pour r√©pondre aux questions
"""

import os
from typing import Dict, List
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from core.config import settings
from core.prompts import SOCRATIC_SYSTEM_PROMPT, RAG_PROMPT_TEMPLATE
import logging

logger = logging.getLogger(__name__)


# Variable globale pour le cache du vectorstore
_vectorstore_cache = None


def load_vectorstore() -> FAISS:
    """
    Charge l'index FAISS depuis le disque
    Utilise un cache pour √©viter de recharger √† chaque requ√™te
    
    Returns:
        FAISS: Index vectoriel charg√©
    """
    global _vectorstore_cache
    
    if _vectorstore_cache is not None:
        logger.debug("üì¶ Utilisation du vectorstore en cache")
        return _vectorstore_cache
    
    index_path = settings.faiss_index_path
    
    if not os.path.exists(index_path):
        raise FileNotFoundError(
            f"‚ùå Index FAISS non trouv√© dans '{index_path}'. "
            f"Veuillez d'abord lancer l'ingestion via POST /ingest"
        )
    
    logger.info(f"üìÇ Chargement de l'index FAISS depuis: {index_path}")
    
    # Cr√©er les m√™mes embeddings que pour l'ingestion
    embeddings = HuggingFaceEmbeddings(
        model_name=settings.embedding_model,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # Charger l'index FAISS
    vectorstore = FAISS.load_local(
        index_path,
        embeddings,
        allow_dangerous_deserialization=True  # N√©cessaire pour FAISS
    )
    
    _vectorstore_cache = vectorstore
    logger.info("‚úÖ Index FAISS charg√© avec succ√®s")
    
    return vectorstore


def get_llm():
    """
    Cr√©e et retourne le LLM configur√© (OpenAI ou Ollama)
    
    Returns:
        LLM instance configur√©e
    """
    if settings.llm_provider == "openai":
        logger.info(f"ü§ñ Utilisation de OpenAI: {settings.openai_model}")
        
        if not settings.openai_api_key:
            raise ValueError("‚ùå OPENAI_API_KEY non configur√©e dans .env")
        
        return ChatOpenAI(
            model=settings.openai_model,
            temperature=settings.openai_temperature,
            api_key=settings.openai_api_key
        )
    
    elif settings.llm_provider == "ollama":
        logger.info(f"ü§ñ Utilisation de Ollama: {settings.ollama_model}")
        
        # Utiliser notre wrapper √©l√©gant Ollama
        from services.ollama_wrapper import OllamaChat
        from langchain_core.runnables import RunnableLambda
        
        # Cr√©er le client Ollama
        ollama_client = OllamaChat(
            model=settings.ollama_model,
            base_url=settings.ollama_base_url,
            temperature=settings.openai_temperature
        )
        
        # Encapsuler dans un RunnableLambda pour compatibilit√© LangChain
        # Ceci permet √† LangChain d'accepter notre wrapper custom
        ollama_runnable = RunnableLambda(ollama_client.invoke)
        
        return ollama_runnable
    
    elif settings.llm_provider == "gemini":
        logger.info(f"üåü Utilisation de Gemini: {os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')}")
        
        # Utiliser notre wrapper Gemini
        from services.gemini_wrapper import GeminiChat
        
        # Cr√©er le client Gemini
        gemini_client = GeminiChat(
            model=os.getenv('GEMINI_MODEL', 'gemini-1.5-flash'),
            temperature=settings.openai_temperature
        )
        
        # Le wrapper Gemini est d√©j√† compatible
        return gemini_client
    
    else:
        raise ValueError(f"‚ùå LLM provider '{settings.llm_provider}' non support√©")




def get_rag_chain():
    """
    Cr√©e et configure la cha√Æne RAG compl√®te avec LangChain
    
    Returns:
        Chain: Cha√Æne de retrieval configur√©e
    """
    logger.info("‚öôÔ∏è  Configuration de la cha√Æne RAG...")
    
    # Charger le vectorstore
    vectorstore = load_vectorstore()
    
    # Cr√©er le retriever
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": settings.retrieval_top_k}
    )
    
    # Cr√©er le LLM
    llm = get_llm()
    
    # Cr√©er le prompt template
    prompt = ChatPromptTemplate.from_messages([
        ("system", SOCRATIC_SYSTEM_PROMPT),
        ("human", RAG_PROMPT_TEMPLATE)
    ])
    
    # Cr√©er la cha√Æne de combinaison de documents
    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    
    # Cr√©er la cha√Æne de retrieval compl√®te
    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)
    
    logger.info("‚úÖ Cha√Æne RAG configur√©e")
    return rag_chain


def format_sources(source_documents: List) -> List[Dict]:
    """
    Formate les documents sources pour la r√©ponse API
    
    Args:
        source_documents: Liste de documents retourn√©s par le retriever
        
    Returns:
        List[Dict]: Liste de sources format√©es
    """
    sources = []
    
    for doc in source_documents:
        source = {
            "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
            "metadata": doc.metadata,
            "page": doc.metadata.get("page", "N/A"),
            "source_file": doc.metadata.get("source_file", "Unknown")
        }
        sources.append(source)
    
    return sources


def ask_question(question: str) -> Dict:
    """
    Point d'entr√©e principal pour poser une question au RAG
    Impl√©mentation custom √©l√©gante qui √©vite les probl√®mes de compatibilit√© LangChain
    
    Args:
        question: Question de l'√©tudiant
        
    Returns:
        Dict: R√©ponse avec answer et sources
    """
    logger.info(f"‚ùì Question re√ßue: {question[:100]}...")
    
    try:
        # √âtape 1: Charger le vectorstore
        vectorstore = load_vectorstore()
        logger.info("‚úÖ Vectorstore charg√©")
        
        # √âtape 2: R√©cup√©rer les documents pertinents (optimis√© pour qualit√©)
        docs = vectorstore.similarity_search(question, k=4)  # 4 docs pour meilleure couverture
        logger.info(f"‚úÖ {len(docs)} documents r√©cup√©r√©s")
        
        # √âtape 3: Construire le contexte depuis les documents (limit√© pour performance)
        context_parts = []
        for i, doc in enumerate(docs, 1):
            source_file = doc.metadata.get('source_file', 'Unknown')
            page = doc.metadata.get('page', 'N/A')
            # Limiter la taille du contenu (500 chars pour √©quilibre qualit√©/vitesse)
            content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
            context_parts.append(
                f"[Document {i} - {source_file}, Page {page}]\n{content}"
            )
        
        context = "\n\n---\n\n".join(context_parts)
        
        # √âtape 4: Construire le prompt complet avec syst√®me + contexte + question
        full_prompt = f"""{SOCRATIC_SYSTEM_PROMPT}

Contexte documentaire (extraits de cours):
{context}

Question de l'√©tudiant: {question}

R√©ponds en suivant les principes socratiques d√©finis dans le prompt syst√®me.
N'oublie pas de citer les sources avec pr√©cision (nom du fichier PDF et num√©ro de page)."""
        
        logger.info("‚úÖ Prompt construit")
        
        # √âtape 5: Appeler le LLM (Ollama ou Gemini)
        if settings.llm_provider == "ollama":
            from services.ollama_wrapper import OllamaChat
            
            ollama = OllamaChat(
                model=settings.ollama_model,
                base_url=settings.ollama_base_url,
                temperature=settings.openai_temperature
            )
            
            logger.info("ü§ñ Appel √† Ollama...")
            response = ollama.invoke(full_prompt)
            answer = response.content
        
        elif settings.llm_provider == "gemini":
            from services.gemini_wrapper import GeminiChat
            
            gemini = GeminiChat(
                model=os.getenv('GEMINI_MODEL', 'gemini-1.5-flash'),
                temperature=settings.openai_temperature
            )
            
            logger.info("üåü Appel √† Gemini...")
            response = gemini.invoke(full_prompt)
            answer = response.content
        
        elif settings.llm_provider == "openai":
            from langchain_openai import ChatOpenAI
            
            llm = ChatOpenAI(
                model=settings.openai_model,
                temperature=settings.openai_temperature,
                api_key=settings.openai_api_key
            )
            
            logger.info("ü§ñ Appel √† OpenAI...")
            response = llm.invoke(full_prompt)
            answer = response.content
        
        else:
            raise ValueError(f"Provider {settings.llm_provider} non support√©")
        
        logger.info("‚úÖ R√©ponse g√©n√©r√©e")
        
        # √âtape 6: Formater les sources
        formatted_sources = format_sources(docs)
        
        # √âtape 7: Construire la r√©ponse finale
        result = {
            "answer": answer,
            "sources": formatted_sources,
            "model_used": settings.ollama_model if settings.llm_provider == "ollama" else settings.openai_model,
            "num_sources": len(formatted_sources)
        }
        
        logger.info(f"‚úÖ R√©ponse compl√®te ({len(formatted_sources)} sources utilis√©es)")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du traitement de la question: {str(e)}")
        raise


def reset_vectorstore_cache():
    """
    R√©initialise le cache du vectorstore
    Utile apr√®s une r√©ingestion
    """
    global _vectorstore_cache
    _vectorstore_cache = None
    logger.info("üîÑ Cache du vectorstore r√©initialis√©")
