"""
Service d'ingestion et de vectorisation des documents PDF
Extrait le texte, le d√©coupe en chunks, et cr√©e l'index FAISS
"""

import os
import shutil
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from core.config import settings
from services.minio_client import download_pdf_files
import logging


logger = logging.getLogger(__name__)


def load_pdfs_from_directory(pdf_dir: str) -> List:
    """
    Charge tous les PDFs d'un r√©pertoire avec PyPDFLoader
    
    Args:
        pdf_dir: Chemin du r√©pertoire contenant les PDFs
        
    Returns:
        List: Liste de documents LangChain
    """
    all_documents = []
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]
    
    logger.info(f"üìÑ Chargement de {len(pdf_files)} fichiers PDF...")
    
    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_dir, pdf_file)
        try:
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            
            # Enrichir les m√©tadonn√©es
            for doc in documents:
                doc.metadata['source_file'] = pdf_file
            
            all_documents.extend(documents)
            logger.info(f"  ‚úì {pdf_file}: {len(documents)} pages charg√©es")
            
        except Exception as e:
            logger.error(f"  ‚úó Erreur lors du chargement de {pdf_file}: {str(e)}")
    
    logger.info(f"‚úÖ Total: {len(all_documents)} pages charg√©es")
    return all_documents


def split_documents(documents: List) -> List:
    """
    D√©coupe les documents en chunks avec RecursiveCharacterTextSplitter
    
    Args:
        documents: Liste de documents LangChain
        
    Returns:
        List: Liste de chunks
    """
    logger.info(f"‚úÇÔ∏è  D√©coupage des documents en chunks (size={settings.chunk_size}, overlap={settings.chunk_overlap})...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.chunk_size,
        chunk_overlap=settings.chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    logger.info(f"‚úÖ {len(chunks)} chunks cr√©√©s")
    
    return chunks


def create_embeddings():
    """
    Cr√©e le mod√®le d'embeddings HuggingFace
    
    Returns:
        HuggingFaceEmbeddings: Mod√®le d'embeddings configur√©
    """
    logger.info(f"üß† Chargement du mod√®le d'embeddings: {settings.embedding_model}")
    
    embeddings = HuggingFaceEmbeddings(
        model_name=settings.embedding_model,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    logger.info("‚úÖ Mod√®le d'embeddings charg√©")
    return embeddings


def create_faiss_index(chunks: List, embeddings) -> FAISS:
    """
    Cr√©e l'index FAISS √† partir des chunks et embeddings
    
    Args:
        chunks: Liste de chunks de documents
        embeddings: Mod√®le d'embeddings
        
    Returns:
        FAISS: Index vectoriel FAISS
    """
    logger.info("üîß Cr√©ation de l'index FAISS...")
    
    vectorstore = FAISS.from_documents(
        documents=chunks,
        embedding=embeddings
    )
    
    logger.info("‚úÖ Index FAISS cr√©√©")
    return vectorstore


def save_faiss_index(vectorstore: FAISS, index_path: str = None):
    """
    Sauvegarde l'index FAISS localement
    
    Args:
        vectorstore: Index FAISS √† sauvegarder
        index_path: Chemin de sauvegarde (par d√©faut: settings.faiss_index_path)
    """
    if index_path is None:
        index_path = settings.faiss_index_path
    
    logger.info(f"üíæ Sauvegarde de l'index FAISS dans: {index_path}")
    
    # Cr√©er le dossier si n√©cessaire
    os.makedirs(index_path, exist_ok=True)
    
    vectorstore.save_local(index_path)
    logger.info("‚úÖ Index FAISS sauvegard√©")


def ingest_documents(use_local_pdfs: bool = False, local_pdf_dir: str = None) -> dict:
    """
    Fonction principale d'ingestion compl√®te:
    1. T√©l√©charge les PDFs depuis MinIO (ou utilise des PDFs locaux)
    2. Extrait le texte
    3. D√©coupe en chunks
    4. Vectorise et cr√©e l'index FAISS
    5. Sauvegarde l'index
    
    Args:
        use_local_pdfs: Si True, utilise les PDFs du dossier local au lieu de MinIO
        local_pdf_dir: Chemin du dossier contenant les PDFs locaux (d√©faut: ./Cours)
        
    Returns:
        dict: Statistiques de l'ingestion
    """
    logger.info("=" * 60)
    logger.info("üöÄ D√âMARRAGE DE L'INGESTION DES DOCUMENTS")
    logger.info("=" * 60)
    
    try:
        # √âtape 1: R√©cup√©rer les PDFs
        if use_local_pdfs:
            # Utiliser les PDFs du dossier "Cours" par d√©faut
            if local_pdf_dir is None:
                local_pdf_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "Cours")
            
            logger.info(f"üìÅ Utilisation des PDFs locaux depuis: {local_pdf_dir}")
            
            if not os.path.exists(local_pdf_dir):
                raise ValueError(f"Le dossier {local_pdf_dir} n'existe pas")
            
            pdf_dir = local_pdf_dir
            pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]
        else:
            # T√©l√©charger depuis MinIO
            pdf_files = download_pdf_files()
            pdf_dir = settings.temp_pdf_dir
        
        if not pdf_files:
            raise ValueError("Aucun fichier PDF trouv√©")
        
        # √âtape 2: Charger les PDFs
        documents = load_pdfs_from_directory(pdf_dir)
        
        if not documents:
            raise ValueError("Aucun document charg√©")
        
        # √âtape 3: D√©couper en chunks
        chunks = split_documents(documents)
        
        # √âtape 4: Cr√©er les embeddings
        embeddings = create_embeddings()
        
        # √âtape 5: Cr√©er l'index FAISS
        vectorstore = create_faiss_index(chunks, embeddings)
        
        # √âtape 6: Sauvegarder l'index
        save_faiss_index(vectorstore)
        
        # Nettoyage: supprimer les PDFs temporaires si t√©l√©charg√©s depuis MinIO
        if not use_local_pdfs and os.path.exists(settings.temp_pdf_dir):
            shutil.rmtree(settings.temp_pdf_dir)
            logger.info("üßπ Fichiers temporaires supprim√©s")
        
        stats = {
            "status": "success",
            "files_processed": len(pdf_files) if isinstance(pdf_files, list) else len(os.listdir(pdf_dir)),
            "total_pages": len(documents),
            "total_chunks": len(chunks),
            "index_path": settings.faiss_index_path
        }
        
        logger.info("=" * 60)
        logger.info("‚úÖ INGESTION TERMIN√âE AVEC SUCC√àS")
        logger.info(f"   üìä Fichiers trait√©s: {stats['files_processed']}")
        logger.info(f"   üìÑ Pages extraites: {stats['total_pages']}")
        logger.info(f"   ‚úÇÔ∏è  Chunks cr√©√©s: {stats['total_chunks']}")
        logger.info(f"   üíæ Index sauvegard√©: {stats['index_path']}")
        logger.info("=" * 60)
        
        return stats
        
    except Exception as e:
        logger.error(f"‚ùå ERREUR LORS DE L'INGESTION: {str(e)}")
        raise
